{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pandas++.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO5AfLwCJzwZ72xJz+EsogV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcgLXaLlgQBE","executionInfo":{"status":"ok","timestamp":1632171698644,"user_tz":240,"elapsed":134,"user":{"displayName":"Michael Bamford","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06117200875806169964"}},"outputId":"cbd9585e-7d99-48d4-efc3-7b6403597014"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn import linear_model as lm\n","\n","#set csv to vars\n","taxData = pd.read_csv(\"sales-and-use-tax.csv\")\n","popData = pd.read_csv(\"popVermontTowns.csv\", header=1)\n","\n","# clean pop data\n","popData = popData.drop('CTC', axis=1)\n","\n","#data engineering steps\n","# prep for merging\n","# reshape the pop data to three columns name, calendar year, and population\n","popData2 = popData.melt(id_vars=['NAME'], var_name=\"Calendar Year\", value_name=\"Population\")\n","\n","# change types of pop data to aline with tax data\n","popData2 = popData2.astype({'Calendar Year': 'int64', 'Population': 'int32'})\n","\n","# merge pop data and tax data\n","mergedData = taxData.merge(popData2, left_on=[\"Town\", \"Calendar Year\"], right_on=[\"NAME\", \"Calendar Year\"], how='inner')\n","\n","#feature we want to predict\n","targetName = 'Gross'\n","DataFeatures = mergedData.drop(targetName, axis=1)\n","# Data mining steps - Using standard deviation\n","mostImportantFeature = DataFeatures.std().sort_values(ascending=False)[0:1]\n","\n","# Testing out choice from Data Mining Results\n","# Setting up input arrays for test\n","# Find most important feature, by our data mining methodology\n","featureName = mostImportantFeature.keys()[0]\n","\n","xb = np.array(mergedData[featureName])\n","xb = xb.reshape(-1,1)\n","yb = np.array(mergedData[targetName])\n","yb = yb.reshape(-1,1)\n","bestModel = lm.LinearRegression().fit(xb, yb)\n","bestModelScore = bestModel.score(xb,yb)\n","\n","# Showing the worst case from Data Mining Results\n","#xw = np.array(mergedData['Calendar Year'])\n","#xw = xw.reshape(-1,1)\n","#yw = np.array(mergedData['Gross'])\n","#yw = yw.reshape(-1,1)\n","#worstModel = lm.LinearRegression().fit(xw, yw)\n","#worstModelScore = worstModel.score(xw,yw)\n","\n","#if the regression score is above threshold\n","BestDataModel = DataFeatures[featureName]\n","\n","print(BestDataModel)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["0      4.890067e+05\n","1      1.857137e+06\n","2      1.431322e+07\n","3      1.672038e+07\n","4      9.368863e+07\n","           ...     \n","780    3.834357e+08\n","781    3.381616e+07\n","782    5.526718e+06\n","783    1.867886e+06\n","784    2.302616e+07\n","Name: Retail, Length: 785, dtype: float64\n"]}]},{"cell_type":"markdown","metadata":{"id":"7WoOVXN6bJya"},"source":["retail has biggest standard deviation of 5.0e+07\n","\n","year has the smallest std of 1.13"]}]}